---
title: 'Session 4: Homework 2'
author: "Study group 5: Jakob Bollenberger, William Byun, Anaanya Chopra, Mengyao
  Dai, Louis Frach, Giorgi Mdivnishvili, Claire Zhang"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---


```{r, setup, include=FALSE}
knitr::opts_chunk$set(
  message = FALSE, 
  warning = FALSE, 
  tidy=FALSE,     # display code as typed
  size="small")   # slightly smaller font for code
options(digits = 3)

# default figure size
knitr::opts_chunk$set(
  fig.width=6.75, 
  fig.height=6.75,
  fig.align = "center"
)
```


```{r load-libraries, include=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(mosaic)
library(ggthemes)
library(lubridate)
library(here)
library(skimr)
library(janitor)
library(httr)
library(readxl)
library(vroom)
library(infer)
library(readr)
library(dplyr)

```



# Climate change and temperature anomalies 


```{r weather_data, cache=TRUE}

weather <- 
  read_csv("https://data.giss.nasa.gov/gistemp/tabledata_v4/NH.Ts+dSST.csv", 
           skip = 1, 
           na = "***")

```



```{r tidyweather}

tidyweather <- weather %>% 
  select(1:13) %>% 
  pivot_longer(cols=2:13,
               names_to="Month",
               values_to="delta")
```



```{r scatter_plot}

tidyweather <- tidyweather %>%
  mutate(date = ymd(paste(as.character(Year), Month, "1")),
         month = month(date, label=TRUE),
         year = year(date))

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(alpha=0.5)+
  geom_smooth(color="red") +
  theme_bw() +
  labs (
    title = "Weather Anomalies", 
    subtitle = "Measured as deviations from base value in degrees celsius", 
    y="Temperature deviation from base value", 
    x="Year"
    )

```

From the graph it becomes very clear that since c. 1975, temperature deviations from the base value have consistently increased. 

```{r facet_wrap, echo=FALSE}

ggplot(tidyweather, aes(x=date, y = delta))+
  geom_point(alpha=0.5)+
  geom_smooth(color="red") +
  theme_bw()+ 
  labs (
    title = "Weather Anomalies",
    subtitle = "Measured as deviations from base value in degrees celsius",
    y="Temperature deviation from base value", 
    x="Year")+
  facet_wrap(~month)
```

These charts show that temperature deviations in individual months vary between each other. For instance, in the winter months (December, January, February), deviations are particularly significant when compared to the summer months (June, July). Especially in areas with a lot of permafrost and icecaps, warmer winters can cause substantial ice-melts. 


```{r intervals, results="hide"}

comparison <- tidyweather %>% 
  filter(Year>= 1881) %>%     #remove years prior to 1881
  #create new variable 'interval', and assign values based on criteria below:
  mutate(interval = case_when(
    Year %in% c(1881:1920) ~ "1881-1920",
    Year %in% c(1921:1950) ~ "1921-1950",
    Year %in% c(1951:1980) ~ "1951-1980",
    Year %in% c(1981:2010) ~ "1981-2010",
    TRUE ~ "2011-present"
  ))

```



```{r density_plot, results="hide"}

ggplot(comparison, aes(x=delta, fill=interval))+
  geom_density(alpha=0.2) +   #density plot with tranparency set to 20%
  theme_bw() +                #theme
  labs (
    title = "Density Plot for Monthly Temperature Anomalies",
    y     = "Density")         #changing y-axis label to sentence case

```
The key message of this graph is that in the most recent time interval (2011-present), monthly temperature anomalies are particularly common. These anomalies are (about) normally distributed around +1 degrees.


```{r averaging, results="hide"}

#creating yearly averages
average_annual_anomaly <- tidyweather %>% 
  group_by(Year) %>%   #grouping data by Year
  
  # creating summaries for mean delta 
  # use `na.rm=TRUE` to eliminate NA (not available) values 
  summarise(annual_average_delta=mean(delta,  na.rm=TRUE)) 
  

#plotting the data:
ggplot(average_annual_anomaly, aes(x=Year, y= annual_average_delta))+
  geom_point()+
  
  #Fit the best fit line, using LOESS method
  geom_smooth(method="loess") +
  
  #change to theme_bw() to have white background + black frame around plot
  theme_bw() +
  labs (
    title = "Average Yearly Anomaly",
    y     = "Average Annual Delta"
  )                         


```




```{r, calculate_CI_using_formula, results="hide"}

formula_ci <- comparison %>%
  filter(interval == "2011-present") %>% 
  summarise(mean=mean(delta, na.rm=TRUE),
            t_critical=qt(0.975,n()-1),
            sd=sd(delta, na.rm=TRUE), 
            count=n(),
            se=sd/(count)^(1/2), 
            lower_CI=mean-t_critical*se, 
            upper_CI=mean+t_critical*se)

formula_ci
```


```{r, calculate_CI_using_bootstrap}

# use the infer package to construct a 95% CI for delta

boot_delta<-comparison %>% 
  filter(interval=="2011-present") %>% 
  specify(response = delta) %>% 
  generate(reps=1000, type="bootstrap") %>% 
  calculate(stat="mean")

percentile_CI<-boot_delta %>% 
  get_confidence_interval(level=0.95, type="percentile")
percentile_CI

```



To find the lower and upper confidence intervals, we generated 1000 artificial samples of data and calculated their respective means. Out of these means, the 25th lowest corresponds to the lower confidence interval (1.01) and the 975th highest corresponds to the upper confidence interval (1.11). Based on this data, we can be 95% confident that the average temperature anomaly of the time period 2011-present lies between 1.01-1.11 degrees. Clearly, this is supportive of climate change.    



```{r, read_global_warming_pew_data}

global_warming_pew <- read_csv(here::here("data_new", "global_warming_pew.csv"))
```




```{r}
global_warming_pew %>% 
  filter(response ==c("Not warming","Earth is warming")) %>% 
  count(party_or_ideology, response)

#4 proportion CIs, where x=nr. of people who believe earth is warming

#Conservative Republican
prop.test(x=124, n=349, conf.level = 0.95)

#Liberal Democrat 
prop.test(x=202, n=213, conf.level = 0.95)

#Mod/Cons Democrat
prop.test(x=281, n=360, conf.level = 0.95)

#Mod/Lib Republican
prop.test(x=68, n=136, conf.level = 0.95)


```

As one would assume, the population proportion estimates indicate that conservative republicans are particularly sceptical towards the notion of a warming climate. In contrast, liberal democrats almost uniformly agree with the claim that the climate is warming. Even among more conservative democrats, a high proportion agrees that the climate is warming. Among moderate republicans, there is a high chance that opinions on the topic are about equally split.    

# Biden's Approval Margins


```{r, cache=TRUE}
# Import approval polls data directly off fivethirtyeight website
approval_polllist <- read_csv('https://projects.fivethirtyeight.com/biden-approval-data/approval_polllist.csv') 

glimpse(approval_polllist)
# Use `lubridate` to fix dates, as they are given as characters.

approval_polllist$modeldate<-mdy(approval_polllist$modeldate) 
approval_polllist$startdate<-mdy(approval_polllist$startdate)
approval_polllist$enddate<-mdy(approval_polllist$enddate)
approval_polllist$createddate<-mdy(approval_polllist$createddate)
 


head(approval_polllist)
class(approval_polllist$modeldate)


```

## Create a plot



```{r trump_margins, echo=FALSE, out.width="100%"}

#getting the data in the right format
approval_polllist_plot<-approval_polllist %>%
  mutate(net_approval=approve-disapprove, na.rm=TRUE) %>%
  mutate(week=isoweek(enddate)) %>% 
  arrange(enddate) %>%
  group_by(week) %>%
  summarise(mean_net_approval=mean(net_approval),
            sd_approval=sd(net_approval, na.rm=TRUE),
            se_approval=sd_approval/sqrt(n()),
            t_critical=qt(0.975,n()-1),
            lower_CI=mean_net_approval-t_critical*se_approval,
            higher_CI=mean_net_approval+t_critical*se_approval)

#plotting the data
ggplot(approval_polllist_plot, aes(x = week, y=mean_net_approval, color="red"))+ 
  geom_point()+
  geom_line() +
  geom_line(aes(y = lower_CI))+
  geom_line(aes(y = higher_CI))+
  geom_hline(yintercept = 0, color="orange")+
  geom_ribbon(aes(x=week, ymax=higher_CI, ymin=lower_CI), fill="grey", alpha=.3)+
  geom_smooth(color="blue", level=0)+
  theme_bw()+
  theme(legend.position = "none")+
  labs(title = "Estimating Approval Margin (approve-disapprove) for Joe Biden",
       subtitle = 'Weekly average of all polls',
       x="Week of the Year",
       y="Average Approval Margin (Approve-Disapprove)")

#original plot  
#knitr::include_graphics(here::here("images", "biden_approval_margin.png"), error = FALSE)
```

## Compare Confidence Intervals



It seems that the data set has been altered from the original data set, because polls for the given data set only start in week 7. Even when considering week 7 as week 1, the 'week 3' and 'week 25' comparison hints at no substantial differences in the confidence intervals. Differences would be greatest if the t_critical for the two weeks differed substantially. This would be the case if the nr. of polls per week (i.e., the sample size n) differed a lot from week 3 to week 25. With a smaller number of polls, a greater range of mean approval ratings is necessary to be 95% certain that the true population mean lies within that range.    


# Challenge 1: Excess rentals in TfL bike sharing


```{r, get_tfl_data, cache=TRUE}
url <- "https://data.london.gov.uk/download/number-bicycle-hires/ac29363e-e0cb-47cc-a97a-e216d900a6b0/tfl-daily-cycle-hires.xlsx"

# Download TFL data to temporary file
httr::GET(url, write_disk(bike.temp <- tempfile(fileext = ".xlsx")))

# Use read_excel to read it as dataframe
bike0 <- read_excel(bike.temp,
                   sheet = "Data",
                   range = cell_cols("A:B"))

# change dates to get year, month, and week
bike <- bike0 %>% 
  clean_names() %>% 
  rename (bikes_hired = number_of_bicycle_hires) %>% 
  mutate (year = year(day),
          month = lubridate::month(day, label = TRUE),
          week = isoweek(day))

years <- c("2016","2017","2018","2019","2020","2021")
bike_recent_years <- bike %>% 
  filter(year %in% years)

bike_expected <- bike %>% filter(year %in% c("2016","2017","2018","2019"))%>% 
  group_by(month) %>% 
  summarize(expected_bikes_hired =mean(bikes_hired))

bike_join <-left_join(bike_recent_years,bike_expected,by="month",all=TRUE)

actual_bike <- bike_join %>% 
  group_by(month,year) %>% 
  summarize(actual_bikes_hired = mean(bikes_hired))

bike_table <- left_join(actual_bike,bike_expected,by="month",all=TRUE) %>% 
  mutate(
    up =ifelse(actual_bikes_hired >expected_bikes_hired,actual_bikes_hired- expected_bikes_hired,0),
    down =ifelse(actual_bikes_hired<expected_bikes_hired, actual_bikes_hired- expected_bikes_hired,0)
  )


##Graph one
graph_one <- ggplot(bike_table,aes(x=month,group=1))+
  geom_line(aes(y=actual_bikes_hired))+
  geom_line(aes(y=expected_bikes_hired),color = "blue")+
  facet_wrap(~year)+
    theme_bw() +
  labs (
    title = "Monthly Changes in Bike Rentals",
    subtitle = "Change from monthly average shown in blue and calculated between 2016 and 2019",
    x="Month",
    y= "Number of bikes"
  )+
  theme(axis.text.x = element_text(angle = 45))+
  geom_ribbon(
    aes(ymin=expected_bikes_hired, ymax=up+expected_bikes_hired), fill="lightsteelblue"
  )+
  geom_ribbon(
    aes(ymax=expected_bikes_hired, ymin=down+expected_bikes_hired),fill="mistyrose1"
  )
   
graph_one
```



```{r tfl_month_year_grid, echo=FALSE, out.width="100%"}
#knitr::include_graphics(here::here("images", "tfl_distributions_monthly.png"), error = FALSE)
```

As expected, the Covid-19 pandemic caused a strong decline in bike rentals in spring of 2020, as people tended to stay at home and not rent bikes to go to work or on excursions. In May and June of 2021, the nr. of bike rentals increased, likely due to a reduction of Covid-19 restrictions and better weather conditions.    

```{r tfl_absolute_monthly_change, echo=FALSE, out.width="100%"}

```



```{r tfl_percent_change, out.width="100%"}
## Graph 2
years <- c("2016","2017","2018","2019","2020","2021")
bike_recent_years <- bike %>% 
  filter(year %in% years)

bike_expected_week <- bike %>% filter(year %in% c("2016","2017","2018","2019"))%>% 
  group_by(week) %>% 
  summarize(expected_bikes_hired =mean(bikes_hired))

bike_join_week<-left_join(bike_recent_years,bike_expected_week,by="week")

actual_bike_week<- bike_join_week %>% 
  group_by(week,year) %>% 
  summarize(actual_bikes_hired = mean(bikes_hired))


bike_table_week <- left_join(actual_bike_week,bike_expected_week,by="week",all=TRUE) %>% 
  mutate(percentage_change = actual_bikes_hired/expected_bikes_hired-1) %>% 
  head(-1) %>% 
  mutate(
    up =ifelse(percentage_change>0,percentage_change,0),
    down =ifelse(percentage_change<0,percentage_change,0)
    
  )

graph_two <- ggplot(bike_table_week,aes(x=week,group=1))+
  geom_line(aes(y=percentage_change))+
  facet_wrap(~year)+
    theme_bw() +
  labs (
    title = "Weekly Changes in Bike Rentals",
    subtitle = "% Change from weekly average calculated between 2016 and 2019",
    x="Week",
    y= "% change")+
      annotate("rect", fill = "grey", alpha = 0.5, 
        xmin = 14, xmax = 26,
        ymin = -Inf, ymax = Inf)+
        annotate("rect", fill = "grey", alpha = 0.5, 
        xmin = 40, xmax = 52,
        ymin = -Inf, ymax = Inf)+
  scale_y_continuous(labels=scales::percent)+
  geom_ribbon(
    aes(ymin=0, ymax=up+0), fill="lightsteelblue"
  )+
geom_ribbon(
    aes(ymax=0, ymin=down+0),fill="mistyrose1"
  )+
geom_rug(aes(colour=ifelse(actual_bikes_hired>=expected_bikes_hired,">=0","<0")),sides="b")+
  scale_colour_manual(values=c("#CB454A","#7DCD85"),name="Actual vs Expected ", guide=FALSE)

graph_two




```

Both mean and median values can provide helpful information when creating these kinds of graphs. With mean values, more weight is given to outlier values. This could be interesting to capture the effect of for example a strike of train drivers. Median values are more helpful when the goal is to show more normalized values (normalized as in less impacted by outlier values).


# Challenge 2: How has the CPI and its components changed over the last few years?


```{r cpi_all_components_since_2016,  out.width="100%"}

cpi_data <- read_csv(here::here("data_new", "cpi_data.csv"))

cpi<-cpi_data %>% 
  separate(title,into=c("generic","title"),sep=": ") %>%
  separate(title,into=c("title", "generic_2"), sep=" in U.S.") %>% 
  mutate(date=dmy(date)) %>% 
  arrange(date) %>%
  filter(year(date)>=2015) %>% 
  group_by(component) %>%
  mutate(year_change=value/lag(value,12) - 1) %>% 
  mutate(max_change=max(year_change, na.rm=TRUE))

cpi$max_change[cpi$title=="All Items"]<-500


ggplot(cpi, aes(x=date, y=year_change))+
  geom_point(aes(x=date, color=year_change>0))+
  geom_smooth()+
  facet_wrap(~reorder(title,desc(max_change)), scales="free")+
  theme( axis.text = element_text( size = 7 ),axis.text.x = element_text( size = 7 ),axis.title = element_text( size = 7),   legend.position="none",strip.text = element_text(size = 7))+
  scale_y_continuous(labels = scales::percent)+
  labs(title = "Yearly change of US CPI (All Items) and its components",
       subtitle = 'YoY change being positive or negative (Jan 2016 to Aug 2021)',
       x="Time",
       y="YoY change")


```



# Details

- Who did you collaborate with: Jakob Bollenberger, William Byun, Anaanya Chopra, Mengyao Dai, Louis Frach, Giorgi Mdivnishvili, Claire Zhang
- Approximately how much time did you spend on this problem set: 8-10 hours
- What, if anything, gave you the most trouble: correct coloring of graphs, applying fct_reorder


